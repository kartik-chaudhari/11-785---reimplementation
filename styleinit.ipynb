{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "styleinit.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRD7ipkRZNOw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import argparse\n",
        "import shutil\n",
        "import torch\n",
        "from torch.backends import cudnn\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQhej1tYaRDV",
        "colab_type": "text"
      },
      "source": [
        "Dataset loader below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPrhGATsZtuD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataset loader \n",
        "\n",
        "class FlatDirectoryImageDataset(Dataset):\n",
        "    \"\"\" pyTorch Dataset wrapper for the generic flat directory images dataset \"\"\"\n",
        "\n",
        "    def __setup_files(self):\n",
        "        \"\"\"\n",
        "        private helper for setting up the files_list\n",
        "        :return: files => list of paths of files\n",
        "        \"\"\"\n",
        "        file_names = os.listdir(self.data_dir)\n",
        "        files = []  # initialize to empty list\n",
        "\n",
        "        for file_name in file_names:\n",
        "            possible_file = os.path.join(self.data_dir, file_name)\n",
        "            if os.path.isfile(possible_file):\n",
        "                files.append(possible_file)\n",
        "\n",
        "        # return the files list\n",
        "        return files\n",
        "\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        \"\"\"\n",
        "        constructor for the class\n",
        "        :param data_dir: path to the directory containing the data\n",
        "        :param transform: transforms to be applied to the images\n",
        "        \"\"\"\n",
        "        # define the state of the object\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # setup the files for reading\n",
        "        self.files = self.__setup_files()\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        compute the length of the dataset\n",
        "        :return: len => length of dataset\n",
        "        \"\"\"\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        obtain the image (read and transform)\n",
        "        :param idx: index of the file required\n",
        "        :return: img => image array\n",
        "        \"\"\"\n",
        "        from PIL import Image\n",
        "\n",
        "        img_file = self.files[idx]\n",
        "\n",
        "        if img_file[-4:] == \".npy\":\n",
        "            # files are in .npy format\n",
        "            img = np.load(img_file)\n",
        "            img = Image.fromarray(img.squeeze(0).transpose(1, 2, 0))\n",
        "\n",
        "        else:\n",
        "            # read the image:\n",
        "            img = Image.open(self.files[idx]).convert('RGB')\n",
        "\n",
        "        # apply the transforms on the image\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if img.shape[0] >= 4:\n",
        "            # ignore the alpha channel\n",
        "            # in the image if it exists\n",
        "            img = img[:3, :, :]\n",
        "\n",
        "        # return the image:\n",
        "        return img\n",
        "\n",
        "\n",
        "class FoldersDistributedDataset(Dataset):\n",
        "    \"\"\" pyTorch Dataset wrapper for folder distributed dataset \"\"\"\n",
        "\n",
        "    def __setup_files(self):\n",
        "        \"\"\"\n",
        "        private helper for setting up the files_list\n",
        "        :return: files => list of paths of files\n",
        "        \"\"\"\n",
        "\n",
        "        dir_names = os.listdir(self.data_dir)\n",
        "        files = []  # initialize to empty list\n",
        "\n",
        "        for dir_name in dir_names:\n",
        "            file_path = os.path.join(self.data_dir, dir_name)\n",
        "            file_names = os.listdir(file_path)\n",
        "            for file_name in file_names:\n",
        "                possible_file = os.path.join(file_path, file_name)\n",
        "                if os.path.isfile(possible_file):\n",
        "                    files.append(possible_file)\n",
        "\n",
        "        # return the files list\n",
        "        return files\n",
        "\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        \"\"\"\n",
        "        constructor for the class\n",
        "        :param data_dir: path to the directory containing the data\n",
        "        :param transform: transforms to be applied to the images\n",
        "        \"\"\"\n",
        "        # define the state of the object\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # setup the files for reading\n",
        "        self.files = self.__setup_files()\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        compute the length of the dataset\n",
        "        :return: len => length of dataset\n",
        "        \"\"\"\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        obtain the image (read and transform)\n",
        "        :param idx: index of the file required\n",
        "        :return: img => image array\n",
        "        \"\"\"\n",
        "        from PIL import Image\n",
        "\n",
        "        # read the image:\n",
        "        img_name = self.files[idx]\n",
        "        if img_name[-4:] == \".npy\":\n",
        "            img = np.load(img_name)\n",
        "            img = Image.fromarray(img.squeeze(0).transpose(1, 2, 0))\n",
        "        else:\n",
        "            img = Image.open(img_name).convert('RGB')\n",
        "\n",
        "        # apply the transforms on the image\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if img.shape[0] >= 4:\n",
        "            # ignore the alpha channel\n",
        "            # in the image if it exists\n",
        "            img = img[:3, :, :]\n",
        "\n",
        "        # return the image:\n",
        "        return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rBTXnpeauzY",
        "colab_type": "text"
      },
      "source": [
        "GAN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P933O253bGNX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import os\n",
        "import datetime\n",
        "import time\n",
        "import timeit\n",
        "import copy\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.functional import interpolate\n",
        "\n",
        "import models.Losses as Losses\n",
        "from data import get_data_loader\n",
        "from models import update_average\n",
        "from models.Blocks import DiscriminatorTop, DiscriminatorBlock, InputBlock, GSynthesisBlock\n",
        "from models.CustomLayers import EqualizedConv2d, PixelNormLayer, EqualizedLinear, Truncation\n",
        "\n",
        "\n",
        "class GMapping(nn.Module):\n",
        "\n",
        "    def __init__(self, latent_size=512, dlatent_size=512, dlatent_broadcast=None,\n",
        "                 mapping_layers=8, mapping_fmaps=512, mapping_lrmul=0.01, mapping_nonlinearity='lrelu',\n",
        "                 use_wscale=True, normalize_latents=True, **kwargs):\n",
        "        \"\"\"\n",
        "        Mapping network used in the StyleGAN paper.\n",
        "        :param latent_size: Latent vector(Z) dimensionality.\n",
        "        # :param label_size: Label dimensionality, 0 if no labels.\n",
        "        :param dlatent_size: Disentangled latent (W) dimensionality.\n",
        "        :param dlatent_broadcast: Output disentangled latent (W) as [minibatch, dlatent_size]\n",
        "                                  or [minibatch, dlatent_broadcast, dlatent_size].\n",
        "        :param mapping_layers: Number of mapping layers.\n",
        "        :param mapping_fmaps: Number of activations in the mapping layers.\n",
        "        :param mapping_lrmul: Learning rate multiplier for the mapping layers.\n",
        "        :param mapping_nonlinearity: Activation function: 'relu', 'lrelu'.\n",
        "        :param use_wscale: Enable equalized learning rate?\n",
        "        :param normalize_latents: Normalize latent vectors (Z) before feeding them to the mapping layers?\n",
        "        :param kwargs: Ignore unrecognized keyword args.\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.latent_size = latent_size\n",
        "        self.mapping_fmaps = mapping_fmaps\n",
        "        self.dlatent_size = dlatent_size\n",
        "        self.dlatent_broadcast = dlatent_broadcast\n",
        "\n",
        "        # Activation function.\n",
        "        act, gain = {'relu': (torch.relu, np.sqrt(2)),\n",
        "                     'lrelu': (nn.LeakyReLU(negative_slope=0.2), np.sqrt(2))}[mapping_nonlinearity]\n",
        "\n",
        "        # Embed labels and concatenate them with latents.\n",
        "        # TODO\n",
        "\n",
        "        layers = []\n",
        "        # Normalize latents.\n",
        "        if normalize_latents:\n",
        "            layers.append(('pixel_norm', PixelNormLayer()))\n",
        "\n",
        "        # Mapping layers. (apply_bias?)\n",
        "        layers.append(('dense0', EqualizedLinear(self.latent_size, self.mapping_fmaps,\n",
        "                                                 gain=gain, lrmul=mapping_lrmul, use_wscale=use_wscale)))\n",
        "        layers.append(('dense0_act', act))\n",
        "        for layer_idx in range(1, mapping_layers):\n",
        "            fmaps_in = self.mapping_fmaps\n",
        "            fmaps_out = self.dlatent_size if layer_idx == mapping_layers - 1 else self.mapping_fmaps\n",
        "            layers.append(\n",
        "                ('dense{:d}'.format(layer_idx),\n",
        "                 EqualizedLinear(fmaps_in, fmaps_out, gain=gain, lrmul=mapping_lrmul, use_wscale=use_wscale)))\n",
        "            layers.append(('dense{:d}_act'.format(layer_idx), act))\n",
        "\n",
        "        # Output.\n",
        "        self.map = nn.Sequential(OrderedDict(layers))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First input: Latent vectors (Z) [mini_batch, latent_size].\n",
        "        x = self.map(x)\n",
        "\n",
        "        # Broadcast -> batch_size * dlatent_broadcast * dlatent_size\n",
        "        if self.dlatent_broadcast is not None:\n",
        "            x = x.unsqueeze(1).expand(-1, self.dlatent_broadcast, -1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class GSynthesis(nn.Module):\n",
        "\n",
        "    def __init__(self, dlatent_size=512, num_channels=3, resolution=1024,\n",
        "                 fmap_base=8192, fmap_decay=1.0, fmap_max=512,\n",
        "                 use_styles=True, const_input_layer=True, use_noise=True, nonlinearity='lrelu',\n",
        "                 use_wscale=True, use_pixel_norm=False, use_instance_norm=True, blur_filter=None,\n",
        "                 structure='linear', **kwargs):\n",
        "        \"\"\"\n",
        "        Synthesis network used in the StyleGAN paper.\n",
        "        :param dlatent_size: Disentangled latent (W) dimensionality.\n",
        "        :param num_channels: Number of output color channels.\n",
        "        :param resolution: Output resolution.\n",
        "        :param fmap_base: Overall multiplier for the number of feature maps.\n",
        "        :param fmap_decay: log2 feature map reduction when doubling the resolution.\n",
        "        :param fmap_max: Maximum number of feature maps in any layer.\n",
        "        :param use_styles: Enable style inputs?\n",
        "        :param const_input_layer: First layer is a learned constant?\n",
        "        :param use_noise: Enable noise inputs?\n",
        "        # :param randomize_noise: True = randomize noise inputs every time (non-deterministic),\n",
        "                                  False = read noise inputs from variables.\n",
        "        :param nonlinearity: Activation function: 'relu', 'lrelu'\n",
        "        :param use_wscale: Enable equalized learning rate?\n",
        "        :param use_pixel_norm: Enable pixel_wise feature vector normalization?\n",
        "        :param use_instance_norm: Enable instance normalization?\n",
        "        :param blur_filter: Low-pass filter to apply when resampling activations. None = no filtering.\n",
        "        :param structure: 'fixed' = no progressive growing, 'linear' = human-readable\n",
        "        :param kwargs: Ignore unrecognized keyword args.\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # if blur_filter is None:\n",
        "        #     blur_filter = [1, 2, 1]\n",
        "\n",
        "        def nf(stage):\n",
        "            return min(int(fmap_base / (2.0 ** (stage * fmap_decay))), fmap_max)\n",
        "\n",
        "        self.structure = structure\n",
        "\n",
        "        resolution_log2 = int(np.log2(resolution))\n",
        "        assert resolution == 2 ** resolution_log2 and resolution >= 4\n",
        "        self.depth = resolution_log2 - 1\n",
        "\n",
        "        self.num_layers = resolution_log2 * 2 - 2\n",
        "        self.num_styles = self.num_layers if use_styles else 1\n",
        "\n",
        "        act, gain = {'relu': (torch.relu, np.sqrt(2)),\n",
        "                     'lrelu': (nn.LeakyReLU(negative_slope=0.2), np.sqrt(2))}[nonlinearity]\n",
        "\n",
        "        # Early layers.\n",
        "        self.init_block = InputBlock(nf(1), dlatent_size, const_input_layer, gain, use_wscale,\n",
        "                                     use_noise, use_pixel_norm, use_instance_norm, use_styles, act)\n",
        "        # create the ToRGB layers for various outputs\n",
        "        rgb_converters = [EqualizedConv2d(nf(1), num_channels, 1, gain=1, use_wscale=use_wscale)]\n",
        "\n",
        "        # Building blocks for remaining layers.\n",
        "        blocks = []\n",
        "        for res in range(3, resolution_log2 + 1):\n",
        "            last_channels = nf(res - 2)\n",
        "            channels = nf(res - 1)\n",
        "            # name = '{s}x{s}'.format(s=2 ** res)\n",
        "            blocks.append(GSynthesisBlock(last_channels, channels, blur_filter, dlatent_size, gain, use_wscale,\n",
        "                                          use_noise, use_pixel_norm, use_instance_norm, use_styles, act))\n",
        "            rgb_converters.append(EqualizedConv2d(channels, num_channels, 1, gain=1, use_wscale=use_wscale))\n",
        "\n",
        "        self.blocks = nn.ModuleList(blocks)\n",
        "        self.to_rgb = nn.ModuleList(rgb_converters)\n",
        "\n",
        "        # register the temporary upsampler\n",
        "        self.temporaryUpsampler = lambda x: interpolate(x, scale_factor=2)\n",
        "\n",
        "    def forward(self, dlatents_in, depth=0, alpha=0., labels_in=None):\n",
        "        \"\"\"\n",
        "            forward pass of the Generator\n",
        "            :param dlatents_in: Input: Disentangled latents (W) [mini_batch, num_layers, dlatent_size].\n",
        "            :param labels_in:\n",
        "            :param depth: current depth from where output is required\n",
        "            :param alpha: value of alpha for fade-in effect\n",
        "            :return: y => output\n",
        "        \"\"\"\n",
        "\n",
        "        assert depth < self.depth, \"Requested output depth cannot be produced\"\n",
        "\n",
        "        if self.structure == 'fixed':\n",
        "            x = self.init_block(dlatents_in[:, 0:2])\n",
        "            for i, block in enumerate(self.blocks):\n",
        "                x = block(x, dlatents_in[:, 2 * (i + 1):2 * (i + 2)])\n",
        "            images_out = self.to_rgb[-1](x)\n",
        "        elif self.structure == 'linear':\n",
        "            x = self.init_block(dlatents_in[:, 0:2])\n",
        "\n",
        "            if depth > 0:\n",
        "                for i, block in enumerate(self.blocks[:depth - 1]):\n",
        "                    x = block(x, dlatents_in[:, 2 * (i + 1):2 * (i + 2)])\n",
        "\n",
        "                residual = self.to_rgb[depth - 1](self.temporaryUpsampler(x))\n",
        "                straight = self.to_rgb[depth](self.blocks[depth - 1](x, dlatents_in[:, 2 * depth:2 * (depth + 1)]))\n",
        "\n",
        "                images_out = (alpha * straight) + ((1 - alpha) * residual)\n",
        "            else:\n",
        "                images_out = self.to_rgb[0](x)\n",
        "        else:\n",
        "            raise KeyError(\"Unknown structure: \", self.structure)\n",
        "\n",
        "        return images_out\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "\n",
        "    def __init__(self, resolution, latent_size=512, dlatent_size=512,\n",
        "                 truncation_psi=0.7, truncation_cutoff=8, dlatent_avg_beta=0.995,\n",
        "                 style_mixing_prob=0.9, **kwargs):\n",
        "        \"\"\"\n",
        "        # Style-based generator used in the StyleGAN paper.\n",
        "        # Composed of two sub-networks (G_mapping and G_synthesis).\n",
        "        :param resolution:\n",
        "        :param latent_size:\n",
        "        :param dlatent_size:\n",
        "        :param truncation_psi: Style strength multiplier for the truncation trick. None = disable.\n",
        "        :param truncation_cutoff: Number of layers for which to apply the truncation trick. None = disable.\n",
        "        :param dlatent_avg_beta: Decay for tracking the moving average of W during training. None = disable.\n",
        "        :param style_mixing_prob: Probability of mixing styles during training. None = disable.\n",
        "        :param kwargs: Arguments for sub-networks (G_mapping and G_synthesis).\n",
        "        \"\"\"\n",
        "\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.style_mixing_prob = style_mixing_prob\n",
        "\n",
        "        # Setup components.\n",
        "        self.num_layers = (int(np.log2(resolution)) - 1) * 2\n",
        "        self.g_mapping = GMapping(latent_size, dlatent_size, dlatent_broadcast=self.num_layers, **kwargs)\n",
        "        self.g_synthesis = GSynthesis(resolution=resolution, **kwargs)\n",
        "\n",
        "        if truncation_psi > 0:\n",
        "            self.truncation = Truncation(avg_latent=torch.zeros(dlatent_size),\n",
        "                                         max_layer=truncation_cutoff,\n",
        "                                         threshold=truncation_psi,\n",
        "                                         beta=dlatent_avg_beta)\n",
        "        else:\n",
        "            self.truncation = None\n",
        "\n",
        "    def forward(self, latents_in, depth, alpha, labels_in=None):\n",
        "        \"\"\"\n",
        "        :param latents_in: First input: Latent vectors (Z) [mini_batch, latent_size].\n",
        "        :param depth: current depth from where output is required\n",
        "        :param alpha: value of alpha for fade-in effect\n",
        "        :param labels_in: Second input: Conditioning labels [mini_batch, label_size].\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        dlatents_in = self.g_mapping(latents_in)\n",
        "\n",
        "        if self.training:\n",
        "            # Update moving average of W(dlatent).\n",
        "            # TODO\n",
        "            if self.truncation is not None:\n",
        "                self.truncation.update(dlatents_in[0, 0].detach())\n",
        "\n",
        "            # Perform style mixing regularization.\n",
        "            if self.style_mixing_prob is not None and self.style_mixing_prob > 0:\n",
        "                latents2 = torch.randn(latents_in.shape).to(latents_in.device)\n",
        "                dlatents2 = self.g_mapping(latents2)\n",
        "                layer_idx = torch.from_numpy(np.arange(self.num_layers)[np.newaxis, :, np.newaxis]).to(\n",
        "                    latents_in.device)\n",
        "                cur_layers = 2 * (depth + 1)\n",
        "                mixing_cutoff = random.randint(1,\n",
        "                                               cur_layers) if random.random() < self.style_mixing_prob else cur_layers\n",
        "                dlatents_in = torch.where(layer_idx < mixing_cutoff, dlatents_in, dlatents2)\n",
        "\n",
        "            # Apply truncation trick.\n",
        "            if self.truncation is not None:\n",
        "                dlatents_in = self.truncation(dlatents_in)\n",
        "\n",
        "        fake_images = self.g_synthesis(dlatents_in, depth, alpha)\n",
        "\n",
        "        return fake_images\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "\n",
        "    def __init__(self, resolution, num_channels=3, fmap_base=8192, fmap_decay=1.0, fmap_max=512,\n",
        "                 nonlinearity='lrelu', use_wscale=True, mbstd_group_size=4, mbstd_num_features=1,\n",
        "                 blur_filter=None, structure='linear', **kwargs):\n",
        "        \"\"\"\n",
        "        Discriminator used in the StyleGAN paper.\n",
        "        :param num_channels: Number of input color channels. Overridden based on dataset.\n",
        "        :param resolution: Input resolution. Overridden based on dataset.\n",
        "        # label_size=0,  # Dimensionality of the labels, 0 if no labels. Overridden based on dataset.\n",
        "        :param fmap_base: Overall multiplier for the number of feature maps.\n",
        "        :param fmap_decay: log2 feature map reduction when doubling the resolution.\n",
        "        :param fmap_max: Maximum number of feature maps in any layer.\n",
        "        :param nonlinearity: Activation function: 'relu', 'lrelu'\n",
        "        :param use_wscale: Enable equalized learning rate?\n",
        "        :param mbstd_group_size: Group size for the mini_batch standard deviation layer, 0 = disable.\n",
        "        :param mbstd_num_features: Number of features for the mini_batch standard deviation layer.\n",
        "        :param blur_filter: Low-pass filter to apply when resampling activations. None = no filtering.\n",
        "        :param structure: 'fixed' = no progressive growing, 'linear' = human-readable\n",
        "        :param kwargs: Ignore unrecognized keyword args.\n",
        "        \"\"\"\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        def nf(stage):\n",
        "            return min(int(fmap_base / (2.0 ** (stage * fmap_decay))), fmap_max)\n",
        "\n",
        "        self.mbstd_num_features = mbstd_num_features\n",
        "        self.mbstd_group_size = mbstd_group_size\n",
        "        self.structure = structure\n",
        "        # if blur_filter is None:\n",
        "        #     blur_filter = [1, 2, 1]\n",
        "\n",
        "        resolution_log2 = int(np.log2(resolution))\n",
        "        assert resolution == 2 ** resolution_log2 and resolution >= 4\n",
        "        self.depth = resolution_log2 - 1\n",
        "\n",
        "        act, gain = {'relu': (torch.relu, np.sqrt(2)),\n",
        "                     'lrelu': (nn.LeakyReLU(negative_slope=0.2), np.sqrt(2))}[nonlinearity]\n",
        "\n",
        "        # create the remaining layers\n",
        "        blocks = []\n",
        "        from_rgb = []\n",
        "        for res in range(resolution_log2, 2, -1):\n",
        "            # name = '{s}x{s}'.format(s=2 ** res)\n",
        "            blocks.append(DiscriminatorBlock(nf(res - 1), nf(res - 2),\n",
        "                                             gain=gain, use_wscale=use_wscale, activation_layer=act,\n",
        "                                             blur_kernel=blur_filter))\n",
        "            # create the fromRGB layers for various inputs:\n",
        "            from_rgb.append(EqualizedConv2d(num_channels, nf(res - 1), kernel_size=1,\n",
        "                                            gain=gain, use_wscale=use_wscale))\n",
        "        self.blocks = nn.ModuleList(blocks)\n",
        "\n",
        "        # Building the final block.\n",
        "        self.final_block = DiscriminatorTop(self.mbstd_group_size, self.mbstd_num_features,\n",
        "                                            in_channels=nf(2), intermediate_channels=nf(2),\n",
        "                                            gain=gain, use_wscale=use_wscale, activation_layer=act)\n",
        "        from_rgb.append(EqualizedConv2d(num_channels, nf(2), kernel_size=1,\n",
        "                                        gain=gain, use_wscale=use_wscale))\n",
        "        self.from_rgb = nn.ModuleList(from_rgb)\n",
        "\n",
        "        # register the temporary downSampler\n",
        "        self.temporaryDownsampler = nn.AvgPool2d(2)\n",
        "\n",
        "    def forward(self, images_in, depth, alpha=1., labels_in=None):\n",
        "        \"\"\"\n",
        "        :param images_in: First input: Images [mini_batch, channel, height, width].\n",
        "        :param labels_in: Second input: Labels [mini_batch, label_size].\n",
        "        :param depth: current height of operation (Progressive GAN)\n",
        "        :param alpha: current value of alpha for fade-in\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        assert depth < self.depth, \"Requested output depth cannot be produced\"\n",
        "\n",
        "        if self.structure == 'fixed':\n",
        "            x = self.from_rgb[0](images_in)\n",
        "            for i, block in enumerate(self.blocks):\n",
        "                x = block(x)\n",
        "            scores_out = self.final_block(x)\n",
        "        elif self.structure == 'linear':\n",
        "            if depth > 0:\n",
        "                residual = self.from_rgb[self.depth - depth](self.temporaryDownsampler(images_in))\n",
        "                straight = self.blocks[self.depth - depth - 1](self.from_rgb[self.depth - depth - 1](images_in))\n",
        "                x = (alpha * straight) + ((1 - alpha) * residual)\n",
        "\n",
        "                for block in self.blocks[(self.depth - depth):]:\n",
        "                    x = block(x)\n",
        "            else:\n",
        "                x = self.from_rgb[-1](images_in)\n",
        "\n",
        "            scores_out = self.final_block(x)\n",
        "        else:\n",
        "            raise KeyError(\"Unknown structure: \", self.structure)\n",
        "\n",
        "        return scores_out\n",
        "\n",
        "\n",
        "class StyleGAN:\n",
        "\n",
        "    def __init__(self, structure, resolution, num_channels, latent_size,\n",
        "                 g_args, d_args, g_opt_args, d_opt_args, loss=\"relativistic-hinge\", drift=0.001,\n",
        "                 d_repeats=1, use_ema=False, ema_decay=0.999, device=torch.device(\"cpu\")):\n",
        "        \"\"\"\n",
        "        Wrapper around the Generator and the Discriminator.\n",
        "        :param structure: 'fixed' = no progressive growing, 'linear' = human-readable\n",
        "        :param resolution: Input resolution. Overridden based on dataset.\n",
        "        :param num_channels: Number of input color channels. Overridden based on dataset.\n",
        "        :param latent_size: Latent size of the manifold used by the GAN\n",
        "        :param g_args: Options for generator network.\n",
        "        :param d_args: Options for discriminator network.\n",
        "        :param g_opt_args: Options for generator optimizer.\n",
        "        :param d_opt_args: Options for discriminator optimizer.\n",
        "        :param loss: the loss function to be used\n",
        "                     Can either be a string =>\n",
        "                          [\"wgan-gp\", \"wgan\", \"lsgan\", \"lsgan-with-sigmoid\",\n",
        "                          \"hinge\", \"standard-gan\" or \"relativistic-hinge\"]\n",
        "                     Or an instance of GANLoss\n",
        "        :param drift: drift penalty for the\n",
        "                      (Used only if loss is wgan or wgan-gp)\n",
        "        :param d_repeats: How many times the discriminator is trained per G iteration.\n",
        "        :param use_ema: boolean for whether to use exponential moving averages\n",
        "        :param ema_decay: value of mu for ema\n",
        "        :param device: device to run the GAN on (GPU / CPU)\n",
        "        \"\"\"\n",
        "\n",
        "        # state of the object\n",
        "        assert structure in ['fixed', 'linear']\n",
        "        self.structure = structure\n",
        "        self.depth = int(np.log2(resolution)) - 1\n",
        "        self.latent_size = latent_size\n",
        "        self.device = device\n",
        "        self.d_repeats = d_repeats\n",
        "\n",
        "        self.use_ema = use_ema\n",
        "        self.ema_decay = ema_decay\n",
        "\n",
        "        # Create the Generator and the Discriminator\n",
        "        self.gen = Generator(num_channels=num_channels,\n",
        "                             resolution=resolution,\n",
        "                             structure=self.structure,\n",
        "                             **g_args).to(self.device)\n",
        "        self.dis = Discriminator(num_channels=num_channels,\n",
        "                                 resolution=resolution,\n",
        "                                 structure=self.structure,\n",
        "                                 **d_args).to(self.device)\n",
        "\n",
        "        # if code is to be run on GPU, we can use DataParallel:\n",
        "        # TODO\n",
        "\n",
        "        # define the optimizers for the discriminator and generator\n",
        "        self.__setup_gen_optim(**g_opt_args)\n",
        "        self.__setup_dis_optim(**d_opt_args)\n",
        "\n",
        "        # define the loss function used for training the GAN\n",
        "        self.drift = drift\n",
        "        self.loss = self.__setup_loss(loss)\n",
        "\n",
        "        # Use of ema\n",
        "        if self.use_ema:\n",
        "            # create a shadow copy of the generator\n",
        "            self.gen_shadow = copy.deepcopy(self.gen)\n",
        "            # updater function:\n",
        "            self.ema_updater = update_average\n",
        "            # initialize the gen_shadow weights equal to the weights of gen\n",
        "            self.ema_updater(self.gen_shadow, self.gen, beta=0)\n",
        "\n",
        "    def __setup_gen_optim(self, learning_rate, beta_1, beta_2, eps):\n",
        "        self.gen_optim = torch.optim.Adam(self.gen.parameters(), lr=learning_rate, betas=(beta_1, beta_2), eps=eps)\n",
        "\n",
        "    def __setup_dis_optim(self, learning_rate, beta_1, beta_2, eps):\n",
        "        self.dis_optim = torch.optim.Adam(self.dis.parameters(), lr=learning_rate, betas=(beta_1, beta_2), eps=eps)\n",
        "\n",
        "    def __setup_loss(self, loss):\n",
        "        if isinstance(loss, str):\n",
        "            loss = loss.lower()  # lowercase the string\n",
        "\n",
        "            if loss == \"standard-gan\":\n",
        "                loss = Losses.StandardGAN(self.dis)\n",
        "            elif loss == \"hinge\":\n",
        "                loss = Losses.HingeGAN(self.dis)\n",
        "            elif loss == \"relativistic-hinge\":\n",
        "                loss = Losses.RelativisticAverageHingeGAN(self.dis)\n",
        "            elif loss == \"logistic\":\n",
        "                loss = Losses.LogisticGAN(self.dis)\n",
        "            else:\n",
        "                raise ValueError(\"Unknown loss function requested\")\n",
        "\n",
        "        elif not isinstance(loss, Losses.GANLoss):\n",
        "            raise ValueError(\"loss is neither an instance of GANLoss nor a string\")\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def __progressive_down_sampling(self, real_batch, depth, alpha):\n",
        "        \"\"\"\n",
        "        private helper for down_sampling the original images in order to facilitate the\n",
        "        progressive growing of the layers.\n",
        "        :param real_batch: batch of real samples\n",
        "        :param depth: depth at which training is going on\n",
        "        :param alpha: current value of the fade-in alpha\n",
        "        :return: real_samples => modified real batch of samples\n",
        "        \"\"\"\n",
        "\n",
        "        from torch.nn import AvgPool2d\n",
        "        from torch.nn.functional import interpolate\n",
        "\n",
        "        if self.structure == 'fixed':\n",
        "            return real_batch\n",
        "\n",
        "        # down_sample the real_batch for the given depth\n",
        "        down_sample_factor = int(np.power(2, self.depth - depth - 1))\n",
        "        prior_down_sample_factor = max(int(np.power(2, self.depth - depth)), 0)\n",
        "\n",
        "        ds_real_samples = AvgPool2d(down_sample_factor)(real_batch)\n",
        "\n",
        "        if depth > 0:\n",
        "            prior_ds_real_samples = interpolate(AvgPool2d(prior_down_sample_factor)(real_batch), scale_factor=2)\n",
        "        else:\n",
        "            prior_ds_real_samples = ds_real_samples\n",
        "\n",
        "        # real samples are a combination of ds_real_samples and prior_ds_real_samples\n",
        "        real_samples = (alpha * ds_real_samples) + ((1 - alpha) * prior_ds_real_samples)\n",
        "\n",
        "        # return the so computed real_samples\n",
        "        return real_samples\n",
        "\n",
        "    def optimize_discriminator(self, noise, real_batch, depth, alpha):\n",
        "        \"\"\"\n",
        "        performs one step of weight update on discriminator using the batch of data\n",
        "        :param noise: input noise of sample generation\n",
        "        :param real_batch: real samples batch\n",
        "        :param depth: current depth of optimization\n",
        "        :param alpha: current alpha for fade-in\n",
        "        :return: current loss (Wasserstein loss)\n",
        "        \"\"\"\n",
        "\n",
        "        real_samples = self.__progressive_down_sampling(real_batch, depth, alpha)\n",
        "\n",
        "        loss_val = 0\n",
        "        for _ in range(self.d_repeats):\n",
        "            # generate a batch of samples\n",
        "            fake_samples = self.gen(noise, depth, alpha).detach()\n",
        "\n",
        "            loss = self.loss.dis_loss(real_samples, fake_samples, depth, alpha)\n",
        "\n",
        "            # optimize discriminator\n",
        "            self.dis_optim.zero_grad()\n",
        "            loss.backward()\n",
        "            self.dis_optim.step()\n",
        "\n",
        "            loss_val += loss.item()\n",
        "\n",
        "        return loss_val / self.d_repeats\n",
        "\n",
        "    def optimize_generator(self, noise, real_batch, depth, alpha):\n",
        "        \"\"\"\n",
        "        performs one step of weight update on generator for the given batch_size\n",
        "        :param noise: input random noise required for generating samples\n",
        "        :param real_batch: batch of real samples\n",
        "        :param depth: depth of the network at which optimization is done\n",
        "        :param alpha: value of alpha for fade-in effect\n",
        "        :return: current loss (Wasserstein estimate)\n",
        "        \"\"\"\n",
        "\n",
        "        real_samples = self.__progressive_down_sampling(real_batch, depth, alpha)\n",
        "\n",
        "        # generate fake samples:\n",
        "        fake_samples = self.gen(noise, depth, alpha)\n",
        "\n",
        "        # Change this implementation for making it compatible for relativisticGAN\n",
        "        loss = self.loss.gen_loss(real_samples, fake_samples, depth, alpha)\n",
        "\n",
        "        # optimize the generator\n",
        "        self.gen_optim.zero_grad()\n",
        "        loss.backward()\n",
        "        # Gradient Clipping\n",
        "        nn.utils.clip_grad_norm_(self.gen.parameters(), max_norm=10.)\n",
        "        self.gen_optim.step()\n",
        "\n",
        "        # if use_ema is true, apply ema to the generator parameters\n",
        "        if self.use_ema:\n",
        "            self.ema_updater(self.gen_shadow, self.gen, self.ema_decay)\n",
        "\n",
        "        # return the loss value\n",
        "        return loss.item()\n",
        "\n",
        "    @staticmethod\n",
        "    def create_grid(samples, scale_factor, img_file):\n",
        "        \"\"\"\n",
        "        utility function to create a grid of GAN samples\n",
        "        :param samples: generated samples for storing\n",
        "        :param scale_factor: factor for upscaling the image\n",
        "        :param img_file: name of file to write\n",
        "        :return: None (saves a file)\n",
        "        \"\"\"\n",
        "        from torchvision.utils import save_image\n",
        "        from torch.nn.functional import interpolate\n",
        "\n",
        "        # upsample the image\n",
        "        if scale_factor > 1:\n",
        "            samples = interpolate(samples, scale_factor=scale_factor)\n",
        "\n",
        "        # save the images:\n",
        "        save_image(samples, img_file, nrow=int(np.sqrt(len(samples))),\n",
        "                   normalize=True, scale_each=True, pad_value=128, padding=1)\n",
        "\n",
        "    def train(self, dataset, num_workers, epochs, batch_sizes, fade_in_percentage, logger, output,\n",
        "              num_samples=36, start_depth=0, feedback_factor=100, checkpoint_factor=1):\n",
        "        \"\"\"\n",
        "        Utility method for training the GAN. Note that you don't have to necessarily use this\n",
        "        you can use the optimize_generator and optimize_discriminator for your own training routine.\n",
        "        :param dataset: object of the dataset used for training.\n",
        "                        Note that this is not the data loader (we create data loader in this method\n",
        "                        since the batch_sizes for resolutions can be different)\n",
        "        :param num_workers: number of workers for reading the data. def=3\n",
        "        :param epochs: list of number of epochs to train the network for every resolution\n",
        "        :param batch_sizes: list of batch_sizes for every resolution\n",
        "        :param fade_in_percentage: list of percentages of epochs per resolution used for fading in the new layer\n",
        "                                   not used for first resolution, but dummy value still needed.\n",
        "        :param logger:\n",
        "        :param output: Output dir for samples,models,and log.\n",
        "        :param num_samples: number of samples generated in sample_sheet. def=36\n",
        "        :param start_depth: start training from this depth. def=0\n",
        "        :param feedback_factor: number of logs per epoch. def=100\n",
        "        :param checkpoint_factor:\n",
        "        :return: None (Writes multiple files to disk)\n",
        "        \"\"\"\n",
        "\n",
        "        assert self.depth <= len(epochs), \"epochs not compatible with depth\"\n",
        "        assert self.depth <= len(batch_sizes), \"batch_sizes not compatible with depth\"\n",
        "        assert self.depth <= len(fade_in_percentage), \"fade_in_percentage not compatible with depth\"\n",
        "\n",
        "        # turn the generator and discriminator into train mode\n",
        "        self.gen.train()\n",
        "        self.dis.train()\n",
        "        if self.use_ema:\n",
        "            self.gen_shadow.train()\n",
        "\n",
        "        # create a global time counter\n",
        "        global_time = time.time()\n",
        "\n",
        "        # create fixed_input for debugging\n",
        "        fixed_input = torch.randn(num_samples, self.latent_size).to(self.device)\n",
        "\n",
        "        # config depend on structure\n",
        "        logger.info(\"Starting the training process ... \\n\")\n",
        "        if self.structure == 'fixed':\n",
        "            start_depth = self.depth - 1\n",
        "        step = 1  # counter for number of iterations\n",
        "        for current_depth in range(start_depth, self.depth):\n",
        "            current_res = np.power(2, current_depth + 2)\n",
        "            logger.info(\"Currently working on depth: %d\", current_depth + 1)\n",
        "            logger.info(\"Current resolution: %d x %d\" % (current_res, current_res))\n",
        "\n",
        "            ticker = 1\n",
        "\n",
        "            # Choose training parameters and configure training ops.\n",
        "            # TODO\n",
        "            data = get_data_loader(dataset, batch_sizes[current_depth], num_workers)\n",
        "\n",
        "            for epoch in range(1, epochs[current_depth] + 1):\n",
        "                start = timeit.default_timer()  # record time at the start of epoch\n",
        "\n",
        "                logger.info(\"Epoch: [%d]\" % epoch)\n",
        "                # total_batches = len(iter(data))\n",
        "                total_batches = len(data)\n",
        "\n",
        "                fade_point = int((fade_in_percentage[current_depth] / 100)\n",
        "                                 * epochs[current_depth] * total_batches)\n",
        "\n",
        "                for (i, batch) in enumerate(data, 1):\n",
        "                    # calculate the alpha for fading in the layers\n",
        "                    alpha = ticker / fade_point if ticker <= fade_point else 1\n",
        "\n",
        "                    # extract current batch of data for training\n",
        "                    images = batch.to(self.device)\n",
        "                    gan_input = torch.randn(images.shape[0], self.latent_size).to(self.device)\n",
        "\n",
        "                    # optimize the discriminator:\n",
        "                    dis_loss = self.optimize_discriminator(gan_input, images, current_depth, alpha)\n",
        "\n",
        "                    # optimize the generator:\n",
        "                    gen_loss = self.optimize_generator(gan_input, images, current_depth, alpha)\n",
        "\n",
        "                    # provide a loss feedback\n",
        "                    if i % int(total_batches / feedback_factor + 1) == 0 or i == 1:\n",
        "                        elapsed = time.time() - global_time\n",
        "                        elapsed = str(datetime.timedelta(seconds=elapsed)).split('.')[0]\n",
        "                        logger.info(\n",
        "                            \"Elapsed: [%s] Step: %d  Batch: %d  D_Loss: %f  G_Loss: %f\"\n",
        "                            % (elapsed, step, i, dis_loss, gen_loss))\n",
        "\n",
        "                        # create a grid of samples and save it\n",
        "                        os.makedirs(os.path.join(output, 'samples'), exist_ok=True)\n",
        "                        gen_img_file = os.path.join(output, 'samples', \"gen_\" + str(current_depth)\n",
        "                                                    + \"_\" + str(epoch) + \"_\" + str(i) + \".png\")\n",
        "\n",
        "                        with torch.no_grad():\n",
        "                            self.create_grid(\n",
        "                                samples=self.gen(fixed_input, current_depth, alpha).detach() if not self.use_ema\n",
        "                                else self.gen_shadow(fixed_input, current_depth, alpha).detach(),\n",
        "                                scale_factor=int(\n",
        "                                    np.power(2, self.depth - current_depth - 1)) if self.structure == 'linear' else 1,\n",
        "                                img_file=gen_img_file,\n",
        "                            )\n",
        "\n",
        "                    # increment the alpha ticker and the step\n",
        "                    ticker += 1\n",
        "                    step += 1\n",
        "\n",
        "                elapsed = timeit.default_timer() - start\n",
        "                elapsed = str(datetime.timedelta(seconds=elapsed)).split('.')[0]\n",
        "                logger.info(\"Time taken for epoch: %s\\n\" % elapsed)\n",
        "\n",
        "                if epoch % checkpoint_factor == 0 or epoch == 1 or epoch == epochs[current_depth]:\n",
        "                    save_dir = os.path.join(output, 'models')\n",
        "                    os.makedirs(save_dir, exist_ok=True)\n",
        "                    gen_save_file = os.path.join(save_dir, \"GAN_GEN_\" + str(current_depth) + \"_\" + str(epoch) + \".pth\")\n",
        "                    dis_save_file = os.path.join(save_dir, \"GAN_DIS_\" + str(current_depth) + \"_\" + str(epoch) + \".pth\")\n",
        "                    gen_optim_save_file = os.path.join(\n",
        "                        save_dir, \"GAN_GEN_OPTIM_\" + str(current_depth) + \"_\" + str(epoch) + \".pth\")\n",
        "                    dis_optim_save_file = os.path.join(\n",
        "                        save_dir, \"GAN_DIS_OPTIM_\" + str(current_depth) + \"_\" + str(epoch) + \".pth\")\n",
        "\n",
        "                    torch.save(self.gen.state_dict(), gen_save_file)\n",
        "                    logger.info(\"Saving the model to: %s\\n\" % gen_save_file)\n",
        "                    torch.save(self.dis.state_dict(), dis_save_file)\n",
        "                    torch.save(self.gen_optim.state_dict(), gen_optim_save_file)\n",
        "                    torch.save(self.dis_optim.state_dict(), dis_optim_save_file)\n",
        "\n",
        "                    # also save the shadow generator if use_ema is True\n",
        "                    if self.use_ema:\n",
        "                        gen_shadow_save_file = os.path.join(\n",
        "                            save_dir, \"GAN_GEN_SHADOW_\" + str(current_depth) + \"_\" + str(epoch) + \".pth\")\n",
        "                        torch.save(self.gen_shadow.state_dict(), gen_shadow_save_file)\n",
        "                        logger.info(\"Saving the model to: %s\\n\" % gen_shadow_save_file)\n",
        "\n",
        "        logger.info('Training completed.\\n')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print('Done.')\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}